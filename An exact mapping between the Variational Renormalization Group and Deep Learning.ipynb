{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Overview Of:\n",
    "## \"An exact mapping between the Variational Renormalization Group and Deep Learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Preliminaries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. __Deep Neural Networks__\n",
    "\n",
    "One of the central goals of modern machine learning research is to learn and extract important features directly from data.\n",
    "\n",
    "A natural question to consider is then, \n",
    "__which set of methods best extract important features directly from data and why?__ \n",
    "\n",
    "One of the most successful methods of feature extraction is that of deep learning. \n",
    "These deep learning algorithms, called deep neural networks (DNNs), are statistical models characterized by their multiple layers of \"neurons\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer of neurons can be viewed as a unique and potentially reduced representation of the structured data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/tree/Pictures/DNN.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of dimensionality reduction, a simple and widely used method is principal component analysis (PCA), which finds the directions of greatest variance in the data set and represents each point by its coordinates along each of these directions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/tree/Pictures/PCA.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Autoencoder as DNN__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, PCA is only successful when the variables of the data are linearly seperable. \n",
    "\n",
    "Thus, there exists a non-linear generalization of PCA that uses an adaptive, multilayer __encoder__ network to transform the high-dimensional data into a low-dimensional code. It then uses a similar __decoder__ network to recover the high-dimensional representation of the data from the low-dimensional code. \n",
    "\n",
    "The learning dynamics of the so called \"__autoencoder__\" is produced by stacking multiple layers of __restricted Boltzmann machines__ (__RBMs__), each having only one layer of feature detectors. This single layer of feature detectors then become visible units for learning the next RBM. This layer-by-layer learning can be repeated as many times as desired and it has been shown that adding another layer always improves a lower bound on the log probability that the model assigns to the data, given appropriate constraints.\n",
    "\n",
    "For each layer of features, the RBM captures strong, high-order correlations between the activities of units in the layer below. For a wide variety of data sets, this is an efficient way to progessively reveal low-dimensional, non-linear structure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/tree/Pictures/autoencoder.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pretraining multiple layers of feature detectors the model is \"__unfolded__\", revealing the encoder and decoder sub-network of the full autoencoder. \n",
    "\n",
    "The global fine-tuning stage then replaces stochastic activities by real-valued deterministic probabilities and uses backpropagation through the whole autoencoder to fine-tune the weights for an optimal reconstruction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Restricted Boltzmann Machine as Bayesian Inference__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the family of distributions given by\n",
    "\n",
    "$$ p(x|\\theta)= exp[\\theta \\cdot x-k(x)-\\Psi(\\theta)] $$\n",
    "\n",
    "where $x$ is a random vector variable, $\\theta$ is a vector parameter, and $k(x)$ corresponds to the measure of $x$, \n",
    "\n",
    "$$ d\\mu(x)=exp[-k(x)]dx $$\n",
    "\n",
    "\n",
    "\n",
    "In Bayesian statistics, one assumes the parameter $\\theta$ is also a random variable subject to its own prior distribution $\\pi(\\theta)$. This allows us to consider the joint probability of $\\theta$ and $x$,\n",
    "\n",
    "$$p(x,\\theta)=exp[\\theta \\cdot x -k(x)-\\Psi'(\\theta)]$$\n",
    "\n",
    "where $$ \\Psi'(\\theta)=\\Psi(\\theta)-log\\pi(\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may further consider an extended family of joint distributions with an additional parameter $W$ as $p(x,\\theta; W)$\n",
    "\n",
    "Here, $x$ serves as information given from the environment, and $\\theta$ represents a higher-order concept which specifies the distribution of $x$. An inference system guesses $\\theta$ from $x$, such that $x$ is generated from $p(x|\\theta)$.\n",
    "\n",
    "The RBM is the stochastic version of the above learning model. It takes the traditional Boltzmann machine, a Markov chain over $x$, with distribution given by\n",
    "\n",
    "$$p(x,c,W)=exp[c\\cdot x-(1/2)x^{T}Wx-\\Psi] $$\n",
    "\n",
    "and divides $x$ and $c$ into two parts $x=(v,h)$, and $c=(a,b)$ where $v$ and $h$ are the visible and hidden layers, respectively and $a$ and $b$ are random vectors representing the bias terms.\n",
    "\n",
    "The stable distribution of an RBM is given by\n",
    "\n",
    "$$ p(v,h,a,b,W) = exp[a\\cdot v+b\\cdot h +h^{T}Wv-\\Psi(a,b,W)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stable probability distributions of $v$ and $h$ are given by the marginal distributions \n",
    "\n",
    "$$ p_V(v)=\\sum_h p(v,h) $$\n",
    "$$ p_H(h)=\\sum_v p(v,h) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the RBM with the Bayesian scheme introduced earlier, we can compare two general cases. \n",
    "\n",
    "Let $m$ be the number of neurons in hidden layer and $n$ be the number of neurons in the visible layer.\n",
    "\n",
    "For $m<n$: $$ \\theta = h $$ $$ x = Wv $$ $$ p(x,\\theta) \\rightarrow p(Wv,h)$$\n",
    "\n",
    "For $m>n$: $$ \\theta = h^TW $$ $$ x = v $$ $$ p(x,\\theta) \\rightarrow p(v,h^TW)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that an RBM can represent the Bayesian mechanism of statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning of RBM\n",
    "\n",
    "__How does an RBM learn?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an RBM having the joint probability distribution  $p(v,h,a,b,W)$, we can write the two conditional distributions as \n",
    "    $$ p(h| \\ v,a,b,W) = \\dfrac{p(v,h,a,b,W)}{p_V(v,a,b,W)}$$\n",
    "and\n",
    "    $$p(v| \\ h,a,b,W) = \\dfrac{p(v,h,a,b,W)}{p_H(h,a,b,W)}$$ \n",
    "    \n",
    "They show the probabilities of activities of one layer given the activities of the other layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $q(v)$ be a probability distribution where $v$ is given from the environment. An RBM is trained by modifying the values of $W$, $a$, and $b$ in the marginal distribution $p_V(v,a,b,W)$ to best approximate $q(v)$. This is done by minimizing the Kullback-Leibler divergence between the two distributions \n",
    "\n",
    "$$ D_{KL}[q(v)  :  p_V(v,a,b,W)] $$\n",
    "\n",
    "The minimizing values for $W$, $a$, and $b$ are given by the maximum likelihood estimator.\n",
    "\n",
    "There is also a rather illuminating geometric interpretation of the this RBM training procedure\n",
    "\n",
    "For example, consider a manifold $S_{V,H}$ consisting of all the joint probability distributions of $v$ and $h$. Furthermore, we consider two submanifolds in $S_{V,H}$, one is the manifold of the RBM $$ M_{V,H}=p(v,h,W) $$ and the other is the data submanifold $$ M_{V,H|q} = q(v)r(h|v) $$ We then consider the KL-divergence between the two submanifolds \n",
    "\n",
    "$$ D_{KL}[M_{V,H|q}:M_{V,H}] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/tree/Pictures/DKLMIN.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__m-projection__\n",
    "\n",
    "Given a non-independent distribution of $p(x,y)$ we search for the independent distribution which is closest to  $p(x,y)$ in the sense of the KL-divergence. This is given by the __m-projection__ of $p(x,y)$ into a manifild of independent distributions of $p(x,y)$. This projection is unique and  given by \n",
    "\n",
    "$$ p'(x,y)=p_x(x)p_Y(y) $$\n",
    "\n",
    "$$D_{KL}[p(x,y):p'(x,y)]=\\int p(x,y) \\ log \\dfrac{p(x,y)}{p'(x,y)}dxdy $$\n",
    "\n",
    "this is the mutual information of two random variables $x$, ad $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e-projection__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the geodesic projection between two manifolds that maxmizes the entropy between two points that minimize the KL-divergence. The family maximizing entropy under linear constraints is an exponential family, hence, __e-projection__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the iterative course-graining architecture produced by the learning dynamics of the autoencoder, is strikingly similar to an iterative and course-graining scheme in physics, namely, the __renormalization group__ (__RG__). The successive RG calculation allows for the extraction of relevant features as the system is examined at different length scales by essentially integrating out (marginalizing over) short distance fluctuations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. __ Renormalization method in statistical physics__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical mechanics describes the equilibrium behavior of thermodynamic systems in terms of probability distributions called _ensembles_, that describe the probability of occurrence of all possible configurations of the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boltzmann showed how to construct an ensemble given knowledge of the energy of each configuration. Direct calculation of ensemble properties is difficult for systems at or near phase transitions. \n",
    "\n",
    "This is because as the phase transition is approached, distance fluctuations in the system become infinitely large, and computing the correlations among the different particles becomes a non-trivial calculation. \n",
    "\n",
    "This problem was solved by the renormalization method; it described a process of averaging the ensemble over small-scale correlations and constructed a new ensemble that described the same system, but with the new spatially averaged variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Block spin transformation__\n",
    "\n",
    "The best method of formulating a renormalization calculation is through the context of the Ising model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, consider an ensemble of particles, the average proerties of any function of a particle's position $q$, and momenta $p$, can be calculated by using a _phase space_ composed of these variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the probability density for observing the system at any point in the phase space be \n",
    "\n",
    "$$ d\\gamma \\  exp\\Big[-\\dfrac{H(p,q)-F}{T}\\Big] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $H$ is the Hamiltonian and $d\\gamma$ is a volume element in the phase space. \n",
    "\n",
    "Then, the average of _any_ function of $q$ and $p$ is given by\n",
    "\n",
    "$$ \\langle X(q,p) \\rangle = \\int d\\gamma \\ X(q,p) \\ exp\\Big[-\\dfrac{H(p,q)-F}{T}\\Big] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $F$ is a constant set to give the proper normalization for the probability\n",
    "\n",
    "$$ exp\\Big[-\\dfrac{F}{T} \\Big] = \\int d\\gamma \\  exp\\Big[-\\dfrac{H(p,q)}{T}\\Big] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar free energy calculation for the Ising model of discrete variables, i.e., spins ($\\sigma$) is given by\n",
    "\n",
    "$$ exp\\Big[-\\dfrac{F_{\\{ K\\}}}{T} \\Big]  \\  = \\  \\sum_{c} exp\\Big[-\\dfrac{H_{\\{K\\}}(c)}{T}\\Big] $$\n",
    "\n",
    "the appearance of $\\{K\\}$ indicates that $H$ and $F$ depend on a group of coupling parameters and $c$ denotes the set of possible configurations of the system, i.e., values of all the statistical variables like $\\sigma$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the summation above we imagine making blocks from a two-dimensional Ising model containing 81 spins. These spins are then broken down into 9 blocks each containing 9 spins. Each one of these blocks is then assigned a new spin whose direction is set by the average of the old spins. We then reformulate the model to be analyzed in terms of the new spin variables.\n",
    "\n",
    "To make that happen, one can pick the new spins to have the same direction as the sum of the old spins in the block and the same magnitude as each of the old spins. The change could then be represented by saying that the distance between the nearest-neighboring lattice sites has gone from its old value $a$ to a new, larger value $a'$\n",
    "\n",
    "$$ a \\rightarrow a' = l\\cdot a $$\n",
    "\n",
    "For example, in the figure the lattice has grown by a factor of three, $l=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/tree/Pictures/blockspin.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically this is represented as \n",
    "\n",
    "$$ exp\\Big[-\\dfrac{H'_{\\{K' \\}}(C)}{T} \\Big]  \\  = \\  \\sum_{c} B(C,c) \\ exp\\Big[-\\dfrac{H_{\\{K \\}}(c)}{T}\\Big] $$\n",
    "\n",
    "where $B$ is a function relating the new spins to the old ones and obeys the following\n",
    "\n",
    "$$ \\sum_{C} B(C,c) = 1$$\n",
    "\n",
    "which ensures that the free energy calculated from the new Hamiltonian $H'$ remains unchanged after the block transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is then to find the function $W$ that gives the new couplings as functions of the old ones: \n",
    "\n",
    "$$ \\{K\\}' = W(\\{K\\}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
